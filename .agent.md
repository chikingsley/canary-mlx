# Project Overview

This workspace contains a comprehensive project for converting NVIDIA NeMo ASR (Automatic Speech Recognition) models to MLX format for Apple Silicon, with a focus on Parakeet and Canary models.

## Project Structure

### Core Components

- **parakeet-mlx/**: Main MLX implementation package for Parakeet models
- **mlx/**: Directory containing converted MLX models
- **nemo_to_mlx.py**: Basic conversion script for NeMo to MLX format
- **Conversion Documentation**: Detailed technical reports on model conversion processes

### Key Technologies

- **Language**: Python 3.10+
- **Framework**: MLX (Apple's machine learning framework)
- **Model Types**: NVIDIA NeMo ASR models (FastConformer, TDT, RNNT, CTC)
- **Target Platform**: Apple Silicon (M1/M2/M3 chips)

## Architecture Overview

### Supported Model Types

1. **Parakeet-TDT**: Token-and-Duration Transducer models
2. **Parakeet-RNNT**: RNN-Transducer models
3. **Parakeet-CTC**: Connectionist Temporal Classification models
4. **Canary**: Multilingual ASR models with Transformer decoders (NEW - conversion ready)

### Core Components

- **Conformer Encoder**: FastConformer architecture with relative positional encoding
- **Decoder Variants**: TDT, RNNT, CTC, and Transformer decoders
- **Audio Preprocessing**: Mel-spectrogram feature extraction
- **Tokenization**: BPE (Byte-Pair Encoding) tokenizers

## Development Guidelines

### Code Organization

- **parakeet_mlx/**: Main package directory
  - `parakeet.py`: Core model implementations
  - `conformer.py`: Conformer encoder implementation
  - `rnnt.py`: RNN-T decoder and joint network
  - `ctc.py`: CTC decoder implementation
  - `attention.py`: Attention mechanisms
  - `audio.py`: Audio preprocessing utilities
  - `tokenizer.py`: Tokenization utilities
  - `utils.py`: Model loading and configuration utilities
  - `cli.py`: Command-line interface

### Model Conversion Process

1. **Extract NeMo Archive**: Unpack .nemo files (tar archives)
2. **Weight Transformation**: Convert PyTorch weights to MLX format
3. **Key Remapping**: Transform NeMo parameter names to MLX conventions
4. **Tensor Permutation**: Adjust tensor dimensions for MLX compatibility
5. **Serialization**: Save as .safetensors format

### Best Practices

#### Model Implementation

- Use dataclasses for configuration objects
- Implement proper caching for streaming inference
- Support both greedy and beam search decoding
- Maintain compatibility with Hugging Face Hub

#### Code Style

- Follow MLX conventions for module structure
- Use type hints consistently
- Implement proper error handling for unsupported models
- Document model architectures and conversion processes

#### Testing and Validation

- Verify model outputs against original NeMo implementations
- Test streaming and batch inference modes
- Validate audio preprocessing pipeline
- Check tokenizer compatibility

### Configuration Management

- Use JSON configuration files compatible with NeMo format
- Support both local and Hugging Face Hub model loading
- Implement proper dtype casting (bfloat16 default)
- Handle model-specific parameters (TDT durations, vocabulary, etc.)

### CLI Usage Patterns

```bash
# Basic transcription
parakeet-mlx audio.wav --model model_path

# Streaming mode
parakeet-mlx audio.wav --model model_path --streaming

# With timestamps
parakeet-mlx audio.wav --model model_path --timestamps
```

### Dependencies

#### Core Dependencies

- `mlx>=0.22.1`: Apple's ML framework
- `numpy>=2.2.5`: Numerical computing
- `librosa>=0.11.0`: Audio processing
- `huggingface-hub>=0.30.2`: Model hub integration

#### Development Dependencies

- `torch`: For model conversion (PyTorch weights)
- `safetensors`: Secure tensor serialization
- `typer`: CLI framework
- `dacite`: Configuration parsing

### Model Conversion Guidelines

#### For New Model Types

1. **Analyze Architecture**: Identify encoder/decoder types
2. **Map Components**: Match NeMo modules to MLX equivalents
3. **Weight Mapping**: Create parameter name translation rules
4. **Test Conversion**: Validate against reference outputs
5. **Update Utils**: Add model detection logic to `utils.py`

#### Common Conversion Patterns

- **Convolution Layers**: Permute dimensions (NCHW â†’ NHWC)
- **LSTM Weights**: Split and rename weight matrices
- **Attention**: Handle relative positional encoding
- **Normalization**: Convert BatchNorm to LayerNorm where needed

### File Naming Conventions

- **Models**: Use original NeMo model names
- **Configs**: `config.json` for model configuration
- **Weights**: `model.safetensors` for MLX weights
- **Tokenizers**: Preserve original tokenizer files

### Error Handling

- Graceful fallback for unsupported model types
- Clear error messages for configuration issues
- Validation of audio input formats
- Memory management for large models

### Performance Considerations

- Use streaming inference for real-time applications
- Implement proper caching for encoder states
- Support batch processing for efficiency
- Optimize memory usage with appropriate dtypes

### Documentation Standards

- Document all model architectures
- Provide conversion examples
- Include performance benchmarks
- Maintain compatibility matrices

## Common Issues and Solutions

### Conversion Issues

- **Dimension Mismatches**: Check tensor permutation rules
- **Missing Weights**: Verify NeMo checkpoint extraction
- **Type Errors**: Ensure proper dtype handling

### Runtime Issues

- **Memory Errors**: Use appropriate batch sizes and dtypes
- **Audio Format**: Ensure 16kHz sampling rate
- **Tokenizer Mismatch**: Verify vocabulary compatibility

### Model Loading

- **Config Errors**: Validate JSON configuration format
- **Path Issues**: Check model file locations
- **Hub Access**: Verify Hugging Face credentials

## Canary Model Conversion (NEW)

### Overview

The project now includes comprehensive support for converting NVIDIA's Canary-1B-Flash model to MLX format. This represents a significant expansion beyond the original Parakeet models.

### Key Features

- **Pydantic-based Configuration**: Robust type-safe configuration management
- **Transformer Decoder Support**: New architecture support beyond TDT/RNNT
- **Multilingual Capabilities**: Support for 10+ languages
- **Rich CLI Interface**: User-friendly conversion with progress tracking
- **Comprehensive Testing**: Full test suite for validation

### Conversion Files

- `canary_to_mlx_converter.py`: Main conversion script with Pydantic models
- `parakeet-mlx/parakeet_mlx/transformer.py`: Transformer decoder implementation
- `parakeet-mlx/parakeet_mlx/canary.py`: Complete Canary model implementation
- `test_canary_conversion.py`: Comprehensive test suite
- `setup_canary_conversion.py`: Environment setup script
- `README_CANARY_CONVERSION.md`: Detailed conversion guide

### Usage

```bash
# Setup environment
python setup_canary_conversion.py

# Test setup
python test_canary_conversion.py

# Convert model
python canary_to_mlx_converter.py canary-1b-flash.nemo
```

### Architecture Mapping

- **Encoder**: Reuses FastConformer mapping from Parakeet (proven)
- **Decoder**: New Transformer decoder mapping (Canary-specific)
- **Tokenization**: Extended vocabulary support (~51K tokens vs ~1K)
- **Multilingual**: Language and task token handling

## Future Development

### Planned Features

- Support for additional NeMo model architectures
- Improved streaming performance
- Multi-language model support
- Integration with MLX ecosystem tools
- Beam search decoding for Canary models
- Cross-attention optimization

### Extension Points

- Custom decoder implementations
- New audio preprocessing options
- Alternative tokenization schemes
- Advanced decoding algorithms
- Additional language support
- Task-specific fine-tuning
